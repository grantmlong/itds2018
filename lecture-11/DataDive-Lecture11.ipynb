{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dive 11: Movie Reviews \n",
    "\n",
    "Today, we'll build a naive bayes classifier to judge whether a movie review is positive or negative. In today's exercise, we'll be using a sample of movie reviews [compiled by Stanford University's CS Department](http://ai.stanford.edu/~amaas/data/sentiment/). \n",
    "\n",
    "*** \n",
    "\n",
    "<img src=\"https://grantmlong.com/teaching/lectures/img/l11_r7.png\" width=600/>\n",
    "\n",
    "Courtesy of [@AmznMovieRevws](https://twitter.com/AmznMovieRevws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "random_state = 20181118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.read_csv('https://grantmlong.com/data/sentiment_corpus.csv')\n",
    "corpus_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect df contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.choice(corpus_df.index.values, 4):\n",
    "    print(corpus_df.label.values[i])\n",
    "    print(corpus_df.text.values[i], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Train and Holdout Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_holdout, y_train, y_holdout = train_test_split(\n",
    "    corpus_df['text'], \n",
    "    corpus_df['label'], test_size=0.1,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "train_df = pd.DataFrame(x_train, columns=['text'])\n",
    "train_df['label'] = y_train\n",
    "\n",
    "holdout_df = pd.DataFrame(x_holdout, columns=['text'])\n",
    "holdout_df['label'] = y_holdout\n",
    "\n",
    "train_df.reset_index(inplace=True)\n",
    "holdout_df.reset_index(inplace=True)\n",
    "\n",
    "print(train_df.shape[0], np.mean((train_df.label=='positive')*1))\n",
    "print(holdout_df.shape[0], np.mean((holdout_df.label=='positive')*1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Function to Vectorize Text\n",
    "The functionality `scikit-learn`'s [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) makes it really easy to transform words to text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_text(df, n_features=1000, max_df=0.50, min_df=50):\n",
    "\n",
    "    vectorizer = CountVectorizer(max_df=max_df, \n",
    "                                 min_df=min_df,\n",
    "                                 max_features=n_features,\n",
    "                                 stop_words='english')\n",
    "\n",
    "    X = (vectorizer.fit_transform(df.text)>0)*1\n",
    "    Y = (df.label=='positive')*1\n",
    "\n",
    "    return X, Y, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Vectorized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, vectorizer = featurize_text(train_df, n_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(\n",
    "    data = X.todense(), \n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "count_df = pd.DataFrame(\n",
    "    data = [word_df.sum(), \n",
    "            word_df.loc[train_df.label=='positive'].sum(),\n",
    "            word_df.loc[train_df.label=='negative'].sum()],\n",
    "    index = ['total_count', 'pos_count', 'neg_count']\n",
    ").transpose()\n",
    "\n",
    "count_df['pos_share'] = count_df.pos_count/count_df.total_count*100\n",
    "count_df['neg_share'] = count_df.neg_count/count_df.total_count*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(count_df[['total_count', 'pos_share', 'neg_share']]\n",
    " .sort_values(by='total_count', ascending=False)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validate Vectorization Parameters\n",
    "Here we use `scikit-learn` to train a naive bayes classifier to rate the sentiment of each movie review. For more details of `scikit-learn` see [here](https://scikit-learn.org/stable/modules/naive_bayes.html). \n",
    "* What are the hyperparameters for this model?\n",
    "* Does it makes sense to train more than one? \n",
    "* What should we expect to see from our learning curves? Will more words overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results(df, n_features=1000, max_df=0.80, min_df=50):\n",
    "    \n",
    "    nbayes = BernoulliNB()\n",
    "    X, Y, _ = featurize_text(df, n_features, max_df, min_df)\n",
    "    \n",
    "    results = []\n",
    "    for train, test in k_fold.split(X):\n",
    "        nbayes.fit(X[train, :], Y[train])\n",
    "        y_predicted = nbayes.predict(X[test, :])\n",
    "        accuracy = accuracy_score(Y[test], y_predicted)\n",
    "        results.append(accuracy)\n",
    "    \n",
    "    return np.mean(results), np.std(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cv_results(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_values = [100, 250, 500, 750, 1000, 1500, 2000, 2500, 3000, 3500, 4000]\n",
    "all_mu = []\n",
    "all_sigma = []\n",
    "\n",
    "for m in hp_values:\n",
    "\n",
    "    mu, sigma = get_cv_results(train_df, n_features=m)\n",
    "    all_mu.append(mu)\n",
    "    all_sigma.append(sigma)\n",
    "    \n",
    "    print(m, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(hp_values, all_mu)\n",
    "plt.ylabel('Cross Validation Accuracy')\n",
    "plt.xlabel('Max Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, vectorizer = featurize_text(train_df, n_features=2500)\n",
    "nbayes = BernoulliNB()\n",
    "nbayes.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Test Feature and Target Variables using the vectorizer we produced above.\n",
    "* Why do we use the vectorizer produced above instead of creating a new one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(holdout_df.text)\n",
    "Y_test = (holdout_df.label=='positive')*1\n",
    "\n",
    "# Generate predicted labels (positive=1)\n",
    "y_predicted = nbayes.predict(X_test)\n",
    "\n",
    "# Generate predicted probabilities\n",
    "y_probpos = nbayes.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect in-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state + 0)\n",
    "for i in np.random.choice(holdout_df.index.values, 4):\n",
    "    print('Index:           %i' % i)\n",
    "    print('Prob. Positive:  %0.3f' % nbayes.predict_proba(X_test[i])[0][1])\n",
    "    print('Label:           %s\\n' % holdout_df.label.values[i])\n",
    "    print(holdout_df.text.values[i], '\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Overall Out of Sample Performance\n",
    "* Based on these results, do you think our classifier is overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_performance(classifier):\n",
    "    y_predicted = classifier.predict(X_test)\n",
    "    print('No. of test samples:   %i' % len(y_predicted))\n",
    "    print('Accuracy:              %0.1f%%' % (accuracy_score(Y_test, y_predicted)*100))\n",
    "    print('Precision:             %0.1f%%' % (precision_score(Y_test, y_predicted)*100))\n",
    "    print('Recall:                %0.1f%%' % (recall_score(Y_test, y_predicted)*100))\n",
    "    \n",
    "report_performance(nbayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine our false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state + 0)\n",
    "for i in np.random.choice(holdout_df.loc[(Y_test==0) & (y_predicted==1)].index.values, 2):\n",
    "    print('Index:           %i' % i)\n",
    "    print('Prob. Positive:  %0.3f' % y_probpos[i])\n",
    "    print('True Label:      %s\\n' % holdout_df.label.values[i])\n",
    "    print(holdout_df.text.values[i], '\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine our false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state + 0)\n",
    "for i in np.random.choice(holdout_df.loc[(Y_test==1) & (y_predicted==0)].index.values, 2):\n",
    "    print('Index:           %i' % i)\n",
    "    print('Prob. Positive:  %0.3f' % nbayes.predict_proba(X_test[i])[0][1])\n",
    "    print('True Label:      %s\\n' % holdout_df.label.values[i])\n",
    "    print(holdout_df.text.values[i], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine borderline cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state + 0)\n",
    "\n",
    "border_threshold = 0.05\n",
    "borderline = ((y_probpos>(0.5-border_threshold)) & \n",
    "              (y_probpos<(0.5+border_threshold)))\n",
    "print('No. on border:   %i' % sum(borderline), '\\n')\n",
    "\n",
    "for i in np.random.choice(holdout_df.loc[borderline].index.values, 2):\n",
    "    print('Index:           %i' % i)\n",
    "    print('Prob. Positive:  %0.3f' % nbayes.predict_proba(X_test[i])[0][1])\n",
    "    print('True Label:      %s\\n' % holdout_df.label.values[i])\n",
    "    print(holdout_df.text.values[i], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Against Other Models\n",
    "* How will our performance with Naive Bayes live up to a logistic regression or gradient boosting machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    random_state=random_state, \n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "logreg.fit(X, Y)\n",
    "report_performance(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "gbm.fit(X, Y)\n",
    "report_performance(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take the classifier for a spin! Review a movie.\n",
    "* Does our naive bayes classifier perform like we'd expect it to? \n",
    "* How does it handle things like sarcasm? Negation?\n",
    "\n",
    "***\n",
    "\n",
    "To evaluate our classifier, we'll also rely on some additional reviews helpfully compiled by [@AmznMovieRevws](https://twitter.com/AmznMovieRevws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_review(example, model):\n",
    "    test = vectorizer.transform([example])\n",
    "    print('Words included:  %i' % test.toarray().sum())\n",
    "    print('Prob. Positive:  %0.3f' % nbayes.predict_proba(test)[0][1])\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "This was a really great movie. One of the best I've seen in a while!\n",
    "'''\n",
    "\n",
    "score_review(example, nbayes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "I hated this move. The plot was so slow and the acting was terrible.\n",
    "'''\n",
    "\n",
    "score_review(example, nbayes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://grantmlong.com/teaching/lectures/img/l11_r5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "False Advertising. Don't be fooled. Although this movie is called \"The Rock\" it has NOTHING to do with WWE sports\n",
    "entertainer Dwayne \"The Rock\" Johnson.\n",
    "'''\n",
    "\n",
    "score_review(example, nbayes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://grantmlong.com/teaching/lectures/img/l11_r2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "Misleading. There are no wolves in this movie.\n",
    "'''\n",
    "\n",
    "score_review(example, nbayes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://grantmlong.com/teaching/lectures/img/l11_r4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "If you were going to see this movie because, like me, you love chocolate and and wanted to learn more about \n",
    "the process prepare to go elsewhere. This movie has no chocolate to offer - no milk chocolate, no dark \n",
    "chocolate, nothing about the abuses in the chocolate industry, and not even any hot chocolate on a cold evening.\n",
    "I wish I were one of the dead characters in the movie, as I would have felt more than I did watching this. \n",
    "'''\n",
    "\n",
    "score_review(example, nbayes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://grantmlong.com/teaching/lectures/img/l11_r1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "Expected a superhero named the tick to bite people and infect them with a series of horrible illnesses. \n",
    "Not impressed.\n",
    "'''\n",
    "\n",
    "score_review(example, nbayes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "There is nothing funny about a tick being a super hero. Ticks cause lyme disease and ruin lives. \n",
    "'''\n",
    "\n",
    "score_review(example, nbayes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
